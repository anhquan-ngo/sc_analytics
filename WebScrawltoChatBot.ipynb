{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoe05GeOIQj7cOUcg0pfef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhquan-ngo/sc_analytics/blob/main/WebScrawltoChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pc5e1G6M50a",
        "outputId": "bee6525b-1998-4d13-9f65-50f442e09572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Dependencies\n",
        "# We use sentence-transformers for free local embeddings (no API key needed)\n",
        "!pip install -q langchain langchain-community faiss-cpu beautifulsoup4 sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Run the Free AI Chatbot\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "# The Built-in Colab AI (No API Key needed)\n",
        "from google.colab import ai\n",
        "\n",
        "# LangChain & Vector Store components\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# --- 1. CRAWLER FUNCTION ---\n",
        "def crawl_website(start_url, max_pages=15):\n",
        "    \"\"\"Crawls the website to find relevant sub-pages.\"\"\"\n",
        "    if not start_url.startswith('http'):\n",
        "        start_url = 'https://' + start_url\n",
        "\n",
        "    domain = urlparse(start_url).netloc\n",
        "    visited = set()\n",
        "    to_visit = [start_url]\n",
        "    found_urls = []\n",
        "\n",
        "    print(f\"\\nğŸ•·ï¸ Crawling {start_url} (Limit: {max_pages} pages)...\")\n",
        "\n",
        "    while to_visit and len(found_urls) < max_pages:\n",
        "        current_url = to_visit.pop(0)\n",
        "        if current_url in visited: continue\n",
        "\n",
        "        try:\n",
        "            response = requests.get(current_url, timeout=5)\n",
        "            # Only process HTML\n",
        "            if response.status_code == 200 and 'text/html' in response.headers.get('Content-Type', ''):\n",
        "                visited.add(current_url)\n",
        "                found_urls.append(current_url)\n",
        "                print(f\"Found: {current_url}\")\n",
        "\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                for link in soup.find_all('a', href=True):\n",
        "                    full_url = urljoin(current_url, link['href']).split('#')[0]\n",
        "                    # Only internal links\n",
        "                    if urlparse(full_url).netloc == domain:\n",
        "                        if full_url not in visited and full_url not in to_visit:\n",
        "                            to_visit.append(full_url)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return found_urls\n",
        "\n",
        "# --- 2. BUILD KNOWLEDGE BASE ---\n",
        "def build_vector_store(urls):\n",
        "    print(f\"\\nğŸ“¥ Loading content from {len(urls)} pages...\")\n",
        "    loader = WebBaseLoader(urls)\n",
        "    docs = loader.load()\n",
        "\n",
        "    print(\"âœ‚ï¸ Splitting text into chunks...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    print(\"ğŸ§  Creating embeddings (running locally on Colab)...\")\n",
        "    # This uses a small, free model from Hugging Face to index the data\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "# --- 3. CHAT FUNCTION ---\n",
        "def start_chatbot():\n",
        "    # A. Get URL\n",
        "    target_url = input(\"\\nEnter the Homepage URL: \")\n",
        "    urls = crawl_website(target_url)\n",
        "\n",
        "    if not urls:\n",
        "        print(\"âŒ No pages found. Check the URL.\")\n",
        "        return\n",
        "\n",
        "    # B. Index Content\n",
        "    vectorstore = build_vector_store(urls)\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # Retrieve top 3 matches\n",
        "\n",
        "    chat_history = []\n",
        "\n",
        "    print(\"\\nâœ… Chatbot Ready! (Powered by google.colab.ai). Type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"You: \")\n",
        "        if user_query.lower() in ['exit', 'quit']:\n",
        "            break\n",
        "\n",
        "        # 1. Retrieve relevant context from the website\n",
        "        relevant_docs = retriever.invoke(user_query)\n",
        "        context_text = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "        # 2. Construct the prompt with context\n",
        "        prompt = f\"\"\"You are a helpful assistant. Answer the user's question based strictly on the context provided below.\n",
        "\n",
        "Context from website:\n",
        "{context_text}\n",
        "\n",
        "Chat History:\n",
        "{chat_history[-3:]}\n",
        "\n",
        "User Question: {user_query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # 3. Generate Answer using the built-in free API\n",
        "        try:\n",
        "            # This is the new built-in function\n",
        "            response = ai.generate_text(prompt, model_name=\"gemini-2.0-flash-lite\")\n",
        "            print(f\"Gemini: {response}\\n\")\n",
        "\n",
        "            chat_history.append(f\"User: {user_query}\\nAI: {response}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response: {e}\")\n",
        "\n",
        "# Run the app\n",
        "start_chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4qoPx_3Olmv",
        "outputId": "c8b7d0a7-520f-47b2-9d63-fc53ee019d31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.text_splitter'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1451994199.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# LangChain & Vector Store components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.text_splitter'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}